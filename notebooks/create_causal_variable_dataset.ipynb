{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# DATASET = \"sequence_2dcubes_grayscale_shapes_0.05_child_noise_25k_skip_render_with_noise_encodings\"\n",
    "# DATASET = \"1dcubes_0.16_child_noise_skip_render\"\n",
    "# DATASET = \"1dcubes_equi_child_noise_skip_render\"\n",
    "# DATASET = \"1dcubes_0.5_child_noise_skip_render\"\n",
    "# DATASET = \"1dcubes_0.16_child_noise_gaussian_skip_render\"\n",
    "# DATASET = \"1dcubes_equi_child_noise_gaussian_skip_render\"\n",
    "# DATASET = \"1dcubes_0.5_child_noise_gaussian_skip_render\"\n",
    "DATASET = (\"2dcubes_equiscale_0.05_child_noise\")\n",
    "# DATASET = (\"2dcubes_equiscale_equi_child_noise\")\n",
    "\n",
    "DATASET_DIR = Path(\"outputs/compressed\") / DATASET\n",
    "\n",
    "ROTATE = True\n",
    "\n",
    "OUT_DIR = Path(\"outputs/compressed\") / (\n",
    "    DATASET + f\"_causal_variables{'_unrotated' if not ROTATE else ''}\"\n",
    ")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SHUFFLE = True\n",
    "\n",
    "CAUSAL_VARIABLES = [\n",
    "    \"pos_x1\",\n",
    "    \"pos_y1\",\n",
    "    \"pos_x2\",\n",
    "    \"pos_y2\",\n",
    "    \"pos_x3\",\n",
    "    \"pos_y3\",\n",
    "]\n",
    "# CAUSAL_VARIABLES = [\n",
    "#     \"pos_y1\",\n",
    "#     \"pos_y2\",\n",
    "#     \"pos_y3\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Index in this list determines value of intervention label\n",
    "INTERVENTIONS = [\"_empty_intervention\"] + CAUSAL_VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dci_train', 'test', 'train', 'val']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = [tag.stem for tag in sorted(DATASET_DIR.iterdir())]\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class IntractableError(Exception):\n",
    "    \"\"\"Exception thrown when quantities are fundamentally intractable\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for encoders and decoders.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_features=2, output_features=2):\n",
    "        super().__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "    def forward(self, inputs, deterministic=False):\n",
    "        \"\"\"\n",
    "        Forward transformation.\n",
    "\n",
    "        In an encoder: takes as input the observed data x and returns the latent representation z.\n",
    "        In a decoder: takes as input the latent representation x and returns the reconstructed data\n",
    "        x.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs : torch.Tensor with shape (batchsize, input_features), dtype torch.float\n",
    "            Data to be encoded or decoded\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        outputs : torch.Tensor with shape (batchsize, output_features), dtype torch.float\n",
    "            Encoded or decoded version of the data\n",
    "        additional_info : None, torch.Tensor, or tuple\n",
    "            Additional information that depends on the kind of encoder. For flow-style transforms,\n",
    "            this is the log of the Jacobian determinant. For VAE encoders, this is the log\n",
    "            likelihood or log posterior. Otherwise, None.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inverse(self, inputs, deterministic=False):\n",
    "        \"\"\"\n",
    "        Inverse transformation, if tractable (otherwise raises an exception).\n",
    "\n",
    "        In a decoder: takes as input the observed data x and returns the latent representation z.\n",
    "        In an encoder: takes as input the latent representation z and returns the reconstructed data\n",
    "        x.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs : torch.Tensor with shape (batchsize, input_features), dtype torch.float\n",
    "            Data to be encoded or decoded\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        outputs : torch.Tensor with shape (batchsize, output_features), dtype torch.float\n",
    "            Encoded or decoded version of the data\n",
    "        additional_info: None or torch.Tensor\n",
    "            Additional information that depends on the kind of encoder. For flow-style transforms,\n",
    "            this is the log of the Jacobian determinant. Otherwise, None.\n",
    "        \"\"\"\n",
    "\n",
    "        raise IntractableError()\n",
    "\n",
    "\n",
    "class Inverse(Encoder):\n",
    "    \"\"\"\n",
    "    Wrapper class that inverts the forward and inverse direction, e.g. turning an encoder into a\n",
    "    decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__(\n",
    "            input_features=base_model.output_features,\n",
    "            output_features=base_model.input_features,\n",
    "        )\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def forward(self, inputs, deterministic=False):\n",
    "        \"\"\"\n",
    "        Forward transformation.\n",
    "\n",
    "        In an encoder: takes as input the observed data x and returns the latent representation z.\n",
    "        In a decoder: takes as input the latent representation x and returns the reconstructed data\n",
    "        x.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs : torch.Tensor with shape (batchsize, input_features), dtype torch.float\n",
    "            Data to be encoded or decoded\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        outputs : torch.Tensor with shape (batchsize, output_features), dtype torch.float\n",
    "            Encoded or decoded version of the data\n",
    "        additional_info : None, torch.Tensor, or tuple\n",
    "            Additional information that depends on the kind of encoder. For flow-style transforms,\n",
    "            this is the log of the Jacobian determinant. For VAE encoders, this is the log\n",
    "            likelihood or log posterior. Otherwise, None.\n",
    "        \"\"\"\n",
    "        return self.base_model.inverse(inputs)\n",
    "\n",
    "    def inverse(self, outputs, deterministic=False):\n",
    "        \"\"\"\n",
    "        Inverse transformation, if tractable (otherwise raises an exception).\n",
    "\n",
    "        In a decoder: takes as input the observed data x and returns the latent representation z.\n",
    "        In an encoder: takes as input the latent representation z and returns the reconstructed data\n",
    "        x.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs : torch.Tensor with shape (batchsize, input_features), dtype torch.float\n",
    "            Data to be encoded or decoded\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        outputs : torch.Tensor with shape (batchsize, output_features), dtype torch.float\n",
    "            Encoded or decoded version of the data\n",
    "        additional_info: None or torch.Tensor\n",
    "            Additional information that depends on the kind of encoder. For flow-style transforms,\n",
    "            this is the log of the Jacobian determinant. Otherwise, None.\n",
    "        \"\"\"\n",
    "        return self.base_model(outputs)\n",
    "\n",
    "\n",
    "class SONEncoder(Encoder):\n",
    "    \"\"\"Deterministic SO(n) encoder / decoder\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, coeffs=None, input_features=2, output_features=2, coeff_std=0.05\n",
    "    ):\n",
    "        super().__init__(input_features, output_features)\n",
    "\n",
    "        # Coefficients\n",
    "        d = self.output_features * (self.output_features - 1) // 2\n",
    "        if coeffs is None:\n",
    "            self.coeffs = nn.Parameter(torch.zeros((d,)))  # (d)\n",
    "            nn.init.normal_(self.coeffs, std=coeff_std)\n",
    "        else:\n",
    "            assert coeffs.shape == (d,)\n",
    "            self.coeffs = nn.Parameter(coeffs)\n",
    "\n",
    "        # Generators\n",
    "        self.generators = torch.zeros((d, self.output_features, self.output_features))\n",
    "\n",
    "    def forward(self, inputs, deterministic=False):\n",
    "        \"\"\"Given observed data, returns latent representation; i.e. encoding.\"\"\"\n",
    "        z = torch.einsum(\"ij,bj->bi\", self._rotation_matrix(), inputs)\n",
    "        logdet = torch.zeros([])\n",
    "        return z, logdet\n",
    "\n",
    "    def inverse(self, inputs, deterministic=False):\n",
    "        \"\"\"Given latent representation, returns observed version; i.e. decoding.\"\"\"\n",
    "        x = torch.einsum(\"ij,bj->bi\", self._rotation_matrix(inverse=True), inputs)\n",
    "        logdet = torch.zeros([])\n",
    "        return x, logdet\n",
    "\n",
    "    def _rotation_matrix(self, inverse=False):\n",
    "        \"\"\"\n",
    "        Low-level function to generate an element of SO(n) by exponentiating the Lie algebra\n",
    "        (skew-symmetric matrices)\n",
    "        \"\"\"\n",
    "\n",
    "        o = torch.zeros(\n",
    "            self.output_features, self.output_features, device=self.coeffs.device\n",
    "        )\n",
    "        i, j = torch.triu_indices(self.output_features, self.output_features, offset=1)\n",
    "        if inverse:\n",
    "            o[i, j] = -self.coeffs\n",
    "            o.T[i, j] = self.coeffs\n",
    "        else:\n",
    "            o[i, j] = self.coeffs\n",
    "            o.T[i, j] = -self.coeffs\n",
    "        a = torch.matrix_exp(o)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopEncoder(SONEncoder):\n",
    "    def __init__(self, input_features=2, output_features=2):\n",
    "        d = output_features * (output_features - 1) // 2\n",
    "        coeffs = torch.zeros((d,))\n",
    "        super().__init__(\n",
    "            input_features=input_features,\n",
    "            output_features=output_features,\n",
    "            coeffs=coeffs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_batch(x_sequence):\n",
    "    return x_sequence.reshape(-1, *x_sequence.shape[2:])  # (B*T, *x_dim)\n",
    "\n",
    "\n",
    "def batch_to_sequence(x, batchsize):\n",
    "    return x.reshape(batchsize, -1, x.shape[-1])  # (B, T, z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SONEncoder()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ROTATE:\n",
    "    encoder = SONEncoder(\n",
    "        input_features=len(CAUSAL_VARIABLES), output_features=len(CAUSAL_VARIABLES)\n",
    "    )\n",
    "else:\n",
    "    encoder = NoopEncoder(\n",
    "        input_features=len(CAUSAL_VARIABLES), output_features=len(CAUSAL_VARIABLES)\n",
    "    )\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dci_train\n",
      "Encoding component 0...\n",
      "Encoding component 1...\n",
      "test\n",
      "Encoding component 0...\n",
      "Encoding component 1...\n",
      "train\n",
      "Encoding component 0...\n",
      "Encoding component 1...\n",
      "val\n",
      "Encoding component 0...\n",
      "Encoding component 1...\n"
     ]
    }
   ],
   "source": [
    "file_format = \"pt\"  # \"npz\" or \"pt\"\n",
    "for tag in tags:\n",
    "    print(tag)\n",
    "    data = dict(np.load(DATASET_DIR / f\"{tag}.npz\"))\n",
    "    sequence_length = data[\"original_latents\"].shape[1]\n",
    "    causal_images = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(sequence_length):\n",
    "            print(f\"Encoding component {i}...\")\n",
    "            z = torch.tensor(data[\"original_latents\"][:, i, :])\n",
    "            causal_images_i, _ = encoder.inverse(z)\n",
    "            causal_images.append(causal_images_i)\n",
    "    causal_images = np.stack(causal_images, axis=1, dtype=np.float32)\n",
    "    data[\"imgs\"] = causal_images\n",
    "    if file_format == \"npz\":\n",
    "        np.savez_compressed(OUT_DIR / f\"{tag}.npz\", **data)\n",
    "    elif file_format == \"pt\":\n",
    "        output = (\n",
    "            torch.tensor(data[\"imgs\"][:, 0, :]),\n",
    "            torch.tensor(data[\"imgs\"][:, 1:, :]),\n",
    "            torch.tensor(data[\"original_latents\"][:, 0, :]),\n",
    "            torch.tensor(data[\"original_latents\"][:, 1:, :]),\n",
    "            torch.tensor(data[\"intervention_labels\"]),\n",
    "            torch.tensor(data[\"intervention_masks\"]),\n",
    "            torch.tensor(data[\"epsilon\"][:, 0, :]),\n",
    "            torch.tensor(data[\"epsilon\"][:, 1:, :]),\n",
    "        )\n",
    "        torch.save(output, OUT_DIR / f\"{tag}_encoded.pt\")\n",
    "    torch.save(encoder.state_dict(), OUT_DIR / f\"encoder.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample fresh data set with 2dcubes SCM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x142dcdcf0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn off torch gradient calculation\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scm(e, WEIGHT1=1, WEIGHT2=0.7, NOISE_SCALE1=6, NOISE_SCALE2=6, NOISE_SCALE3=0.3):\n",
    "    pos_x1 = e[:, 0] * NOISE_SCALE1\n",
    "    pos_y1 = e[:, 1] * NOISE_SCALE1\n",
    "    pos_x2 = e[:, 2] * NOISE_SCALE2\n",
    "    pos_y2 = e[:, 3] * NOISE_SCALE2\n",
    "    pos_x3 = (pos_x1 * WEIGHT1 + pos_x2 * WEIGHT2) / (WEIGHT1 + WEIGHT2) + e[\n",
    "        :, 4\n",
    "    ] * NOISE_SCALE3\n",
    "    pos_y3 = (pos_y1 * WEIGHT1 + pos_y2 * WEIGHT2) / (WEIGHT1 + WEIGHT2) + e[\n",
    "        :, 5\n",
    "    ] * NOISE_SCALE3\n",
    "    return torch.stack([pos_x1, pos_y1, pos_x2, pos_y2, pos_x3, pos_y3], dim=1)\n",
    "\n",
    "\n",
    "def intervention_mechanism(e, NOISE_SCALE=6):\n",
    "    return e * NOISE_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(shape, low=-1, high=1):\n",
    "    return (low - high) * torch.rand(shape) + high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_interventions = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1500 samples per intervention for train...\n",
      "Generating 150 samples per intervention for val...\n",
      "Generating 150 samples per intervention for test...\n",
      "Generating 150 samples per intervention for dci_train...\n"
     ]
    }
   ],
   "source": [
    "tags = [\"train\", \"val\", \"test\", \"dci_train\"]\n",
    "for tag, num_samples_per_intervention in zip(tags, [1500, 150, 150, 150]):\n",
    "    print(\n",
    "        f\"Generating {num_samples_per_intervention} samples per intervention for {tag}...\"\n",
    "    )\n",
    "    x1s = []\n",
    "    x2s = []\n",
    "    e1s = []\n",
    "    e2s = []\n",
    "    z1s = []\n",
    "    z2s = []\n",
    "    intervention_labels = []\n",
    "    intervention_masks = []\n",
    "    for intervention in range(7):\n",
    "        e1 = uniform((num_samples_per_intervention, len(CAUSAL_VARIABLES)))\n",
    "        z1 = scm(e1)\n",
    "        x1 = encoder.inverse(z1)[0]\n",
    "        if intervention == 0:\n",
    "            e2 = e1.clone().detach()\n",
    "            z2 = z1.clone().detach()\n",
    "            x2 = x1.clone().detach()\n",
    "        else:\n",
    "            e2 = e1.clone().detach()\n",
    "            # sample new noise for intervened variable\n",
    "            e2[:, intervention - 1] = uniform((num_samples_per_intervention,))\n",
    "            z2 = scm(e2)\n",
    "            # overwrite intervened variable with other mechanism\n",
    "            z2[:, intervention - 1] = intervention_mechanism(e2[:, intervention - 1])\n",
    "            x2 = encoder.inverse(z2)[0]\n",
    "        x1s.append(x1)\n",
    "        x2s.append(x2)\n",
    "        e1s.append(e1)\n",
    "        e2s.append(e2)\n",
    "        z1s.append(z1)\n",
    "        z2s.append(z2)\n",
    "        intervention_labels.append(\n",
    "            torch.ones(num_samples_per_intervention) * intervention\n",
    "        )\n",
    "        # one only for intervened variable\n",
    "        intervention_mask_for_intervention = torch.zeros(\n",
    "            (num_samples_per_intervention, len(CAUSAL_VARIABLES))\n",
    "        )\n",
    "        intervention_mask_for_intervention[:, intervention - 1] = 1\n",
    "        intervention_masks.append(intervention_mask_for_intervention)\n",
    "\n",
    "    x1s = torch.cat(x1s, dim=0)\n",
    "    x2s = torch.cat(x2s, dim=0)\n",
    "    e1s = torch.cat(e1s, dim=0)\n",
    "    e2s = torch.cat(e2s, dim=0)\n",
    "    z1s = torch.cat(z1s, dim=0)\n",
    "    z2s = torch.cat(z2s, dim=0)\n",
    "    intervention_labels = torch.cat(intervention_labels, dim=0)\n",
    "    intervention_masks = torch.cat(intervention_masks, dim=0)\n",
    "\n",
    "    output = (\n",
    "        x1s,\n",
    "        x2s,\n",
    "        z1s,\n",
    "        z2s,\n",
    "        intervention_labels,\n",
    "        intervention_masks,\n",
    "        e1s,\n",
    "        e2s,\n",
    "    )\n",
    "\n",
    "    shuffle = True\n",
    "    if shuffle:\n",
    "        idx = torch.randperm(x1s.shape[0])\n",
    "        output = tuple([o[idx] for o in output])\n",
    "\n",
    "    OUT_DIR = Path(\"outputs/compressed\") / \"2d_cubes_generated\"\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(output, OUT_DIR / f\"{tag}_encoded.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10000 samples per intervention for train...\n",
      "Generating 1000 samples per intervention for val...\n",
      "Generating 1000 samples per intervention for test...\n",
      "Generating 1000 samples per intervention for dci_train...\n"
     ]
    }
   ],
   "source": [
    "tags = [\"train\", \"val\", \"test\", \"dci_train\"]\n",
    "intervention_ratios = [0.1, 0.1, 0.1, 0.1, 0.1, 0.25, 0.25]\n",
    "for tag, total_num_samples in zip(tags, [10000, 1000, 1000, 1000]):\n",
    "    print(f\"Generating {total_num_samples} samples per intervention for {tag}...\")\n",
    "    x1s = []\n",
    "    x2s = []\n",
    "    e1s = []\n",
    "    e2s = []\n",
    "    z1s = []\n",
    "    z2s = []\n",
    "    intervention_labels = []\n",
    "    intervention_masks = []\n",
    "    for intervention in range(7):\n",
    "        num_samples_for_intervention = int(\n",
    "            total_num_samples * intervention_ratios[intervention]\n",
    "        )\n",
    "        e1 = uniform((num_samples_for_intervention, len(CAUSAL_VARIABLES)))\n",
    "        z1 = scm(e1)\n",
    "        x1 = encoder.inverse(z1)[0]\n",
    "        if intervention == 0:\n",
    "            e2 = e1.clone().detach()\n",
    "            z2 = z1.clone().detach()\n",
    "            x2 = x1.clone().detach()\n",
    "        else:\n",
    "            e2 = e1.clone().detach()\n",
    "            # sample new noise for intervened variable\n",
    "            e2[:, intervention - 1] = uniform((num_samples_for_intervention,))\n",
    "            z2 = scm(e2)\n",
    "            # overwrite intervened variable with other mechanism\n",
    "            z2[:, intervention - 1] = intervention_mechanism(e2[:, intervention - 1])\n",
    "            x2 = encoder.inverse(z2)[0]\n",
    "        x1s.append(x1)\n",
    "        x2s.append(x2)\n",
    "        e1s.append(e1)\n",
    "        e2s.append(e2)\n",
    "        z1s.append(z1)\n",
    "        z2s.append(z2)\n",
    "        intervention_labels.append(\n",
    "            torch.ones(num_samples_for_intervention) * intervention\n",
    "        )\n",
    "        # one only for intervened variable\n",
    "        intervention_mask_for_intervention = torch.zeros(\n",
    "            (num_samples_for_intervention, len(CAUSAL_VARIABLES))\n",
    "        )\n",
    "        intervention_mask_for_intervention[:, intervention - 1] = 1\n",
    "        intervention_masks.append(intervention_mask_for_intervention)\n",
    "\n",
    "    x1s = torch.cat(x1s, dim=0)\n",
    "    x2s = torch.cat(x2s, dim=0)\n",
    "    e1s = torch.cat(e1s, dim=0)\n",
    "    e2s = torch.cat(e2s, dim=0)\n",
    "    z1s = torch.cat(z1s, dim=0)\n",
    "    z2s = torch.cat(z2s, dim=0)\n",
    "    intervention_labels = torch.cat(intervention_labels, dim=0)\n",
    "    intervention_masks = torch.cat(intervention_masks, dim=0)\n",
    "\n",
    "    output = (\n",
    "        x1s,\n",
    "        x2s,\n",
    "        z1s,\n",
    "        z2s,\n",
    "        intervention_labels,\n",
    "        intervention_masks,\n",
    "        e1s,\n",
    "        e2s,\n",
    "    )\n",
    "\n",
    "    shuffle = True\n",
    "    if shuffle:\n",
    "        idx = torch.randperm(x1s.shape[0])\n",
    "        output = tuple([o[idx] for o in output])\n",
    "\n",
    "    OUT_DIR = Path(\"outputs/compressed\") / \"2d_cubes_generated_imbalanced\"\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(output, OUT_DIR / f\"{tag}_encoded.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1s, x2s, z1s, z2s, intervention_labels, intervention_masks, e1s, e2s = torch.load(\n",
    "    OUT_DIR / f\"train_encoded.pt\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-images--w8JQbgW-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
